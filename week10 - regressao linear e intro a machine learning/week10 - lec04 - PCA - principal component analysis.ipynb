{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "A matemática de Principal Component Analysis (PCA) é um tanto complexa para ser explicada de maneira simples, pois requer conhecimento de Algebra Linear e Cadeias de Markov.\n",
    "<br><br>\n",
    "\n",
    "Aqui vamos tentar explicar na forma geométrica. Lembre-se na regressão linear, depois de predizer a reta com menor erro quadrático, obtinhamos os resíduos. Caso os resíduos se aproximassem-se de uma distribuição normal, a soma dos erros tenderia a zero.\n",
    "\n",
    "Em PCA fazemos, de certa maneira a mesma coisa, fazemos a mesma coisa. A reta verde grande, abaixo, é um regessão na direção de maior variãncia. Após encontrá-la, há resíduos, e podemos fazer uma regessão na direção ortogonal, a reta em verde de menor tamanho.\n",
    "\n",
    "Caso tenhamos n dimensões, fazemos regressões, ou melhor acamos vetores no espaço na maior dispersão, e assim por diante até a menor dispersão, caso haja. Este vetores são todos ortonormais (perpendicurlares) e na Algebra Linear acompanha um auto-valor. Ou seja, para n dimensões temos n autovetores e n autovalores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA](../figure/pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vetor pode ser escrito como:\n",
    "    \n",
    "$y_i = a_{i1} * x_1  +  a_{i2} * x_2 + .. a_{in} * x_n$\n",
    "<br><br>\n",
    "como se observa, o autovetor y_i é linear, e é uma combinação linear de todos os eixo x1, x2 .. até xn\n",
    "\n",
    "caso os 3 primeiros autovetores, p.ex., perfizerem a soma de 92% de toda a dispersão dos valores neste espaço amostral de n dimensões, podemos aproximar um regressor com 3 autovetores, dispensando n-3 autovetores. A isto denominamos \"redução de dimensionalidade\".\n",
    "\n",
    "Espera-se que um PCA atinja mais de 90% de dispersão total com 2 ou 3 variávies, neste caso podemos ver os grupos no espaço 2D ou 3D.\n",
    "\n",
    "Vamos ver na prática como isto funciona, não se esquecendo que o PCA é totalmente linear e capaz de fazer redução de dimensionalidade, ou seja, clusterizar grupos com muito menos de n autovetores, também chamados de Principais Componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd # pandas e seu alias pd\n",
    "import numpy as np  # numpy  e seu alias np\n",
    "\n",
    "from   scipy import stats\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.insert(1, '../libs/')\n",
    "from stat_lib import *\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt # matplotlib e seu alias plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a tabela de Iris de Fisher & Anderson\n",
    "  - vamos pegar as 4 colunas de dados \n",
    "  - e ver como o PCA clusteriza\n",
    "  - ou seja, um PCA é linear, e não precisa de labels, logo é uma técnica de clusterização **NÃO SUPERVISIONADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../data/Iris.csv\"\n",
    "if os.path.exists(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "else:\n",
    "    print(\"Impossível abrir '%s'\"%(fname))\n",
    "    raise('Error')\n",
    "    \n",
    "print(df.shape)\n",
    "df.columns = ['id', 'sepal_len', 'sepal_width', 'petal_len', 'petal_width', 'species']\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = df[ ['sepal_len', 'sepal_width', 'petal_len', 'petal_width']]\n",
    "\n",
    "pca = PCA()\n",
    "fit = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = df.species.to_list()\n",
    "colors = ['red', 'blue', 'green']\n",
    "specColors = ['red']*50 + ['blue']*50 + ['green']*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.scatter(fit[:,0], fit[:,1], s=100, c=specColors, alpha=1)\n",
    "\n",
    "# patches = [ mpatches.Patch(color=specColors[i], label=species[i]) for i in range(len(specColors)) ]\n",
    "\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.title(\"PCA clusterizing Iris\");\n",
    "# plt.legend(handles=patches, loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de acuracidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma = 0; vals = []; count = 0\n",
    "for val in pca.explained_variance_ratio_:\n",
    "    soma += val\n",
    "    vals.append(soma)\n",
    "    count += 1\n",
    "    print('PCA%d, variance explanation = %.3f, total var.exp = %.3f'%(count, val, soma) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(14,12))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# PCA 3 first components\n",
    "varx = fit[:,0]\n",
    "vary = fit[:,1]\n",
    "varz = fit[:,2]\n",
    "\n",
    "ax.scatter( varx, vary, varz, c=specColors)\n",
    "\n",
    "ax.set_xlabel(\"pc1\")\n",
    "ax.set_ylabel(\"pc2\")\n",
    "ax.set_zlabel(\"pc3\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
